{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = 'C:/Users/Sakshee/Documents/DATASETS/Boschs Traffic Sign Recognition/images_color'\n",
    "test_dir = 'C:/Users/Sakshee/Documents/DATASETS/Boschs Traffic Sign Recognition/images_test'\n",
    "category_names = os.listdir(data_dir)\n",
    "num_classes = len(category_names)\n",
    "print(num_classes)\n",
    "\n",
    "images = []\n",
    "for category in category_names:\n",
    "    folder = data_dir + '/' + category\n",
    "    images.append(len(os.listdir(folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(category_names, images).set_title(\"Number of training images per category:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from PIL import Image\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "img_size = 32\n",
    "channels = 3\n",
    "\n",
    "for i in range(num_classes):\n",
    "    path = os.path.join(data_dir, str(i))\n",
    "    images = os.listdir(path)\n",
    "    \n",
    "    for a in images:\n",
    "        image = Image.open(path + '/' + a)\n",
    "        image = image.resize((img_size, img_size))\n",
    "        image = np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(i)           \n",
    "\n",
    "data = np.array(data)      \n",
    "labels = np.array(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3)\n",
      "(39209,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31367, 32, 32, 3) (7842, 32, 32, 3) (31367,) (7842,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train/255 \n",
    "X_val = X_val/255\n",
    "\n",
    "print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31367, 43)\n",
      "(7842, 43)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train=keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_val= keras.utils.to_categorical(Y_val, num_classes)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class SpatialTransformer(Layer):\n",
    "\n",
    "    def __init__(self, localization_net, output_size, **kwargs):\n",
    "        self.locnet = localization_net\n",
    "        self.output_size = output_size\n",
    "        super(SpatialTransformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.locnet.build(input_shape)\n",
    "        self.trainable_weights = self.locnet.trainable_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_size = self.output_size\n",
    "        return (None,\n",
    "                int(output_size[0]),\n",
    "                int(output_size[1]),\n",
    "                int(input_shape[-1]))\n",
    "\n",
    "    def call(self, X, mask=None):\n",
    "        affine_transformation = self.locnet.call(X)\n",
    "        output = self._transform(affine_transformation, X, self.output_size)\n",
    "        return output\n",
    "\n",
    "    def _repeat(self, x, num_repeats):\n",
    "        ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "        x = tf.reshape(x, shape=(-1,1))\n",
    "        x = tf.matmul(x, ones)\n",
    "        return tf.reshape(x, [-1])\n",
    "\n",
    "    def _interpolate(self, image, x, y, output_size):\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        height = tf.shape(image)[1]\n",
    "        width = tf.shape(image)[2]\n",
    "        num_channels = tf.shape(image)[3]\n",
    "\n",
    "        x = tf.cast(x , dtype='float32')\n",
    "        y = tf.cast(y , dtype='float32')\n",
    "\n",
    "        height_float = tf.cast(height, dtype='float32')\n",
    "        width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "        output_height = output_size[0]\n",
    "        output_width  = output_size[1]\n",
    "\n",
    "        x = .5*(x + 1.0)*(width_float)\n",
    "        y = .5*(y + 1.0)*(height_float)\n",
    "\n",
    "        x0 = tf.cast(tf.floor(x), 'int32')\n",
    "        x1 = x0 + 1\n",
    "        y0 = tf.cast(tf.floor(y), 'int32')\n",
    "        y1 = y0 + 1\n",
    "\n",
    "        max_y = tf.cast(height - 1, dtype='int32')\n",
    "        max_x = tf.cast(width - 1,  dtype='int32')\n",
    "        zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "        x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "        x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "        y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "        y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "        flat_image_dimensions = width*height\n",
    "        pixels_batch = tf.range(batch_size)*flat_image_dimensions\n",
    "        flat_output_dimensions = output_height*output_width\n",
    "        base = self._repeat(pixels_batch, flat_output_dimensions)\n",
    "        base_y0 = base + y0*width\n",
    "        base_y1 = base + y1*width\n",
    "        indices_a = base_y0 + x0\n",
    "        indices_b = base_y1 + x0\n",
    "        indices_c = base_y0 + x1\n",
    "        indices_d = base_y1 + x1\n",
    "\n",
    "        flat_image = tf.reshape(image, shape=(-1, num_channels))\n",
    "        flat_image = tf.cast(flat_image, dtype='float32')\n",
    "        pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "        pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "        pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "        pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "\n",
    "        x0 = tf.cast(x0, 'float32')\n",
    "        x1 = tf.cast(x1, 'float32')\n",
    "        y0 = tf.cast(y0, 'float32')\n",
    "        y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "        area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "        area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "        area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "        area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "        output = tf.add_n([area_a*pixel_values_a,\n",
    "                           area_b*pixel_values_b,\n",
    "                           area_c*pixel_values_c,\n",
    "                           area_d*pixel_values_d])\n",
    "        return output\n",
    "\n",
    "    def _meshgrid(self, height, width):\n",
    "        x_linspace = tf.linspace(-1., 1., width)\n",
    "        y_linspace = tf.linspace(-1., 1., height)\n",
    "        x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "        x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
    "        y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
    "        ones = tf.ones_like(x_coordinates)\n",
    "        indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "        return indices_grid\n",
    "\n",
    "    def _transform(self, affine_transformation, input_shape, output_size):\n",
    "        batch_size = tf.shape(input_shape)[0]\n",
    "        height = tf.shape(input_shape)[1]\n",
    "        width = tf.shape(input_shape)[2]\n",
    "        num_channels = tf.shape(input_shape)[3]\n",
    "\n",
    "        affine_transformation = tf.reshape(affine_transformation, shape=(batch_size,2,3))\n",
    "\n",
    "        affine_transformation = tf.reshape(affine_transformation, (-1, 2, 3))\n",
    "        affine_transformation = tf.cast(affine_transformation, 'float32')\n",
    "\n",
    "        width = tf.cast(width, dtype='float32')\n",
    "        height = tf.cast(height, dtype='float32')\n",
    "        output_height = output_size[0]\n",
    "        output_width = output_size[1]\n",
    "        indices_grid = self._meshgrid(output_height, output_width)\n",
    "        indices_grid = tf.expand_dims(indices_grid, 0)\n",
    "        indices_grid = tf.reshape(indices_grid, [-1]) # flatten?\n",
    "        indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "        indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
    "\n",
    "        # transformed_grid = tf.batch_matmul(affine_transformation, indices_grid)\n",
    "        transformed_grid = tf.matmul(affine_transformation, indices_grid)\n",
    "        x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "        y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "        x_s_flatten = tf.reshape(x_s, [-1])\n",
    "        y_s_flatten = tf.reshape(y_s, [-1])\n",
    "\n",
    "        transformed_image = self._interpolate(input_shape,\n",
    "                                                x_s_flatten,\n",
    "                                                y_s_flatten,\n",
    "                                                output_size)\n",
    "\n",
    "        transformed_image = tf.reshape(transformed_image, shape=(batch_size,\n",
    "                                                                output_height,\n",
    "                                                                output_width,\n",
    "                                                                num_channels))\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Dropout, Flatten, Lambda, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def locnet():\n",
    "    b = np.zeros((2, 3), dtype='float32')\n",
    "    b[0, 0] = 1\n",
    "    b[1, 1] = 1\n",
    "    W = np.zeros((64, 6), dtype='float32')\n",
    "    weights = [W, b.flatten()]\n",
    "    locnet = Sequential()\n",
    "\n",
    "    locnet.add(Conv2D(16, (7, 7), padding='valid', input_shape=(img_size, img_size, channels)))\n",
    "    locnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    locnet.add(Conv2D(32, (5, 5), padding='valid'))\n",
    "    locnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    locnet.add(Conv2D(64, (3, 3), padding='valid'))\n",
    "    locnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    locnet.add(Flatten())\n",
    "    locnet.add(Dense(128))\n",
    "    locnet.add(Activation('elu'))\n",
    "    locnet.add(Dense(64))\n",
    "    locnet.add(Activation('elu'))\n",
    "    locnet.add(Dense(6, weights=weights))\n",
    "\n",
    "    return locnet\n",
    "\n",
    "def conv_model(input_shape=(img_size, img_size, channels)):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(\n",
    "        lambda x: x/127.5 - 1.,\n",
    "        input_shape=(img_size, img_size, channels),\n",
    "        output_shape=(img_size, img_size, channels)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=l2(0.05)))\n",
    "    model.add(LeakyReLU(alpha=0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(3, (1, 1), padding='same', kernel_regularizer=l2(0.05)))\n",
    "    model.add(LeakyReLU(alpha=0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialTransformer(localization_net=locnet(), output_size=(img_size, img_size)))\n",
    "    model.add(Conv2D(16, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(96, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(192, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.05)))\n",
    "    model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have shape (1,) but got array with shape (43,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5ddb27dc7221>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 shuffle=True,)\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m                 batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have shape (1,) but got array with shape (43,)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "\n",
    "model = conv_model()\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_data=(X_val, Y_val),\n",
    "                shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    path = os.path.join(test_dir, str(i))\n",
    "    images = os.listdir(path)\n",
    "    \n",
    "    for a in images:\n",
    "        image = cv2.imread(path + '/' + a)\n",
    "        image = cv2.resize(image, (img_size, img_size))\n",
    "        image = np.array(image)\n",
    "        test_data.append(image)\n",
    "        test_labels.append(i)           \n",
    "\n",
    "test_data = np.array(test_data)      \n",
    "test_labels = np.array(test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
