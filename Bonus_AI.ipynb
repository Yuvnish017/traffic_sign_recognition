{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vry489_fGFR",
        "outputId": "e9c021e6-0ee7-4ca9-d4de-395039a91e5f"
      },
      "source": [
        "import gdown\n",
        "!gdown --id 1-0OQTzDJmEu4ndbXari5K4X5rVPleCyA\n",
        "!gdown --id 1rQttSf0csdtAYQoP0k0uF4eQ3s5EXyGg\n",
        "!gdown --id 1kyNDGnhP-HamLj_3cQuIeB7n5lbwvzDG\n",
        "!gdown --id 1USg1paVac5TSNcZ17vd5U87x_EH-QKn6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-0OQTzDJmEu4ndbXari5K4X5rVPleCyA\n",
            "To: /content/images_color.csv\n",
            "517MB [00:03, 139MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rQttSf0csdtAYQoP0k0uF4eQ3s5EXyGg\n",
            "To: /content/test_set_in_csv.csv\n",
            "127MB [00:00, 178MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kyNDGnhP-HamLj_3cQuIeB7n5lbwvzDG\n",
            "To: /content/Sakshee_GTSRB_classification.h5\n",
            "10.9MB [00:00, 90.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1USg1paVac5TSNcZ17vd5U87x_EH-QKn6\n",
            "To: /content/history.csv\n",
            "100% 4.24k/4.24k [00:00<00:00, 3.39MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkabZfhJgclW"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.models import Model, load_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8WD3FJkpW7K"
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 50\n",
        "classes = 43"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeAYtqiChKEg"
      },
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "with open('/content/images_color.csv', 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    if row[0]!='':\n",
        "      label = row[0]\n",
        "      image = np.array([int(a) for a in row[1:]], dtype='uint8')\n",
        "      image = image.reshape((32, 32, 3))\n",
        "      X_train.append(image)\n",
        "      Y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train).astype(\"uint8\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4FzapHihLcM"
      },
      "source": [
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "with open('/content/test_set_in_csv.csv', 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  header = next(reader)\n",
        "  for row in reader:\n",
        "    if len(row)!=0:\n",
        "      label = row[0]\n",
        "      image = np.array([int(a) for a in row[1:]], dtype='uint8')\n",
        "      image = image.reshape((32, 32, 3))\n",
        "      X_test.append(image)\n",
        "      Y_test.append(label)\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test).astype(\"uint8\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcWbMWKhjni8"
      },
      "source": [
        "model = load_model(\"/content/Sakshee_GTSRB_classification.h5\")\n",
        "\n",
        "ytest_p = model.predict(X_test, verbose=0)\n",
        "ytrain_p = model.predict(X_train, verbose=0)\n",
        "\n",
        "ytest_p = np.argmax(ytest_p, axis=1)\n",
        "ytrain_p = np.argmax(ytrain_p, axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "x7-WaoQirVrJ",
        "outputId": "9b2e937c-1852-44c8-c80f-909c906db6b7"
      },
      "source": [
        "history = pd.read_csv(\"/content/history.csv\", usecols=[\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]) # loss history\n",
        "history.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.721426</td>\n",
              "      <td>0.803872</td>\n",
              "      <td>0.175010</td>\n",
              "      <td>0.949719</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.151956</td>\n",
              "      <td>0.954347</td>\n",
              "      <td>0.119504</td>\n",
              "      <td>0.964051</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.120899</td>\n",
              "      <td>0.963478</td>\n",
              "      <td>0.129041</td>\n",
              "      <td>0.963734</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.094399</td>\n",
              "      <td>0.970568</td>\n",
              "      <td>0.142423</td>\n",
              "      <td>0.963338</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.077100</td>\n",
              "      <td>0.976026</td>\n",
              "      <td>0.093284</td>\n",
              "      <td>0.973078</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  val_loss  val_accuracy  epoch\n",
              "0  0.721426  0.803872  0.175010      0.949719      1\n",
              "1  0.151956  0.954347  0.119504      0.964051      2\n",
              "2  0.120899  0.963478  0.129041      0.963734      3\n",
              "3  0.094399  0.970568  0.142423      0.963338      4\n",
              "4  0.077100  0.976026  0.093284      0.973078      5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp1d0nHO85d4"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "unique, counts = np.unique(Y_test, return_counts=True)\n",
        "class_num = dict(zip(unique, counts))\n",
        "\n",
        "mdict = {\"accuracy\" : accuracy_score(Y_test, ytest_p),\n",
        "         \"recall\" : recall_score(Y_test, ytest_p, average=None),        # accuracy per class\n",
        "         \"precision\" : precision_score(Y_test, ytest_p, average=None),\n",
        "         \"f1-score\" : f1_score(Y_test, ytest_p, average=None),\n",
        "         \"confusion_matrix\" : confusion_matrix(Y_test, ytest_p),\n",
        "         \"history\" : history,\n",
        "         \"class_distribution\" : class_num}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it8Uge3w2W3I"
      },
      "source": [
        "def bonusAI(mdict):\n",
        "  pdict = {}\n",
        "  # overfitting\n",
        "  thresh_over = 0.05\n",
        "  if mdict['accuracy'] > 0.9:\n",
        "    if mdict[\"history\"][\"accuracy\"].iloc[-1] > (thresh_over + mdict[\"history\"][\"val_accuracy\"].iloc[-1]):   # training accu > thresh + validation accu\n",
        "      pdict[\"Your model is overfitting\"]=['add more dropout layers/ batchnormlization layers',\n",
        "                                          'increase dropout rate', \n",
        "                                          'decrease number of epochs',\n",
        "                                          'try early stopping',\n",
        "                                          'add regularization',\n",
        "                                          'try data augmentation']\n",
        "              \n",
        "  # underfitting\n",
        "  thresh_under = 0.9\n",
        "  if mdict[\"history\"][\"accuracy\"].iloc[-1] < thresh_under:                                  # training accu < thresh\n",
        "    pdict[\"Your model is underfitting\"]=['Increase number of epochs',\n",
        "                                         'Reduce dropout layers and their rate',\n",
        "                                         'Increase The Complexity Of The Model', \n",
        "                                         'Increasing the number of layers in the model',\n",
        "                                         'Increasing the number of neurons in layers',]\n",
        "  \n",
        "  # loss fluctuation and slow convergence\n",
        "  thresh_lr1 = 0.03\n",
        "  thresh_lr2 = 0.08\n",
        "  if mdict[\"history\"][\"loss\"].iloc[15:].std() > thresh_lr1:                                 # variance of loss after some epochs < thresh\n",
        "    pdict[\"Your training loss is fluctuating\"]=[\"decrease learning-rate\",\n",
        "                                                \"decrease batch-size\",\n",
        "                                                \"if you are using simple or sgd optimizer then switch to rmsprop or adam(adam is best among all)\",\n",
        "                                                \"normalize dataset if not normalized\",\n",
        "                                                \"try adding dropout layers\"]\n",
        "  elif mdict[\"history\"][\"loss\"].iloc[:15].mean() > thresh_lr2:                              # mean of loss for beginning epochs > thresh\n",
        "    pdict[\"Your training loss convergence rate is slow\"]=[\"increase learning rate\",\n",
        "                                                          \"increase batch-size\"]\n",
        "  \n",
        "  # f1-score, recall, precision\n",
        "  thresh1 = 0.95\n",
        "  thresh2 = 0.95\n",
        "  thresh3 = 0.95\n",
        "  thresh4 = 500\n",
        "\n",
        "  for i in range(len(mdict[\"class_distribution\"].keys())):\n",
        "    if mdict[\"f1-score\"][i] < thresh1:                                          # less performence for i class\n",
        "      if mdict[\"recall\"][i] < thresh2:                                          # less recall == more false negative\n",
        "        if class_num[i] < thresh4:\n",
        "          pdict[\"Your dataset contain less number of data points for class {}\".format(i)]=[\"Oversample class {}\".format(i)]\n",
        "        else:\n",
        "          pdict[\"Your dataset contain too much noise or less quality data points in class{}\".format(i)]=[\"Try undersampling and removing noise for class {}\".format(i)]\n",
        "      elif mdict[\"precision\"][i] < thresh3:                                     # less precision == more false positive\n",
        "        lst = mdict[\"confusion_matrix\"][:,i]\n",
        "        lstt = sorted(list(enumerate(lst)), key=lambda x: x[1])[:2]\n",
        "        pdict[\"class {} is very similar to {} and {} classes\".format(i,lstt[0],lstt[1])]=[\"Try adding data with complex augmentation for all 3 classes\"]\n",
        "\n",
        "  return pdict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIQIVW6y1bHV",
        "outputId": "7dd2012e-434b-4a5c-b552-f401df450d3f"
      },
      "source": [
        "import json\n",
        "\n",
        "sdict = bonusAI(mdict)\n",
        "suggestions = json.dumps(sdict, indent = 6, separators =(\". \", \" - \"))\n",
        "print(suggestions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "      \"Your training loss convergence rate is slow\" - [\n",
            "            \"increase learning rate\". \n",
            "            \"increase batch-size\"\n",
            "      ]. \n",
            "      \"Your dataset contain less number of data points for class 6\" - [\n",
            "            \"Oversample class 6\"\n",
            "      ]. \n",
            "      \"class 20 is very similar to (0, 0) and (1, 0) classes\" - [\n",
            "            \"Try adding data with complex augmentation for all 3 classes\"\n",
            "      ]. \n",
            "      \"Your dataset contain less number of data points for class 22\" - [\n",
            "            \"Oversample class 22\"\n",
            "      ]. \n",
            "      \"Your dataset contain less number of data points for class 26\" - [\n",
            "            \"Oversample class 26\"\n",
            "      ]. \n",
            "      \"Your dataset contain less number of data points for class 27\" - [\n",
            "            \"Oversample class 27\"\n",
            "      ]. \n",
            "      \"class 29 is very similar to (0, 0) and (1, 0) classes\" - [\n",
            "            \"Try adding data with complex augmentation for all 3 classes\"\n",
            "      ]. \n",
            "      \"Your dataset contain less number of data points for class 40\" - [\n",
            "            \"Oversample class 40\"\n",
            "      ]\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkQE-bew6uwY"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}